## README
One of the essential components of Statistical Machine Translation is high quality reference translation. The traditional method of obtaining high translation quality is by employing professional translators which proves to be very expensive and time consuming. It is possible to decrease the cost by outsourcing the translation effort to non-professionals. However naively considering these translations would lead to lower quality, inconsistent and biased results. The aim of this project is to the translations generated by non-professionals along with the metadata associated with it to obtain near professional level translation quality by using multiple crowdsourced translations.

The code is structured as follows:

1. data-train/: This is the training data which you can use to train your model. You will have:
	1. LDCtranslations.tsv - 1369 reference translations
	2. turk_translations.tsv -  1792 source sentences and 4 translations each 
	3. worker_metadata.tsv - worker metadata for 51 workers
    4. features_lmprobs_ter.tsv - bigram and trigram probabilities and average TER values for each translation. 
2. default: This folder contains default.py which is a naive system that chooses the first candidate translation. It takes the path to the turker translations in the data directory as an argument. The output is the first turker translation.
python default.py  > default.out
3. baseline: A skeleton of the baseline solution has been provided for you
4. compute-bleu.py and bleu.py : Objective functions. It takes the path to LDC translations in the data directory as an argument, output of the translation system as an input, and outputs the BLEU score.
python compute-bleu < default.out
