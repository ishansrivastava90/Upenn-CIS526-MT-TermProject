#!/usr/bin/env python
import optparse
from collections import namedtuple
import sys
import os
import math

sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'parsing'))
import parse

import mod_bleu
from utility import *
from numpy import mean

def length_penalty():

    bprob = [ float(h)*penalty[ind] for (ind,h) in enumerate(hyp[4:8]) ];
    tprob = [ float(h)*penalty[ind] for (ind,h) in enumerate(hyp[8:12]) ];

def create_features(feats, hyp, sents):
    features = []
    avg_len = mean([len(s) for s in sents])
    penalty = math.exp(abs(len(hyp)-avg_len)*1.0)

    features.extend([penalty * feat for feat in feats])
    
    # print "feats {} avg len {} for {} penalty {}".format(feats, avg_len, sents, penalty)

    features.append(penalty)

    if len(hyp) <= 2 or "NO TRANSLATION FOUND" in " ".join(hyp):
        # print "hyp {}".format(" ".join(hyp))
        features = [-1000, -1000, penalty]

    return features

def calculate_pro_weights(all_hyps, ref, opts):
    num_sents = len(all_hyps) / 4
    # num_sents = 1
    # num_sents = (len(all_hyps) / 100) + 1
    # print "num of sents is {}".format(num_sents)
    nbests = [[] for _ in xrange(0, num_sents)]

    for s in xrange(0, num_sents):
        #collect all the n-best for i
        hyps_for_one_sent = all_hyps[s * 4:s * 4 + 4]

        nbests[s] = []
        for single_hyp in hyps_for_one_sent:
            hyp = single_hyp.turk_translation.strip().lower().split()
            features = create_features([single_hyp.bigram_prob, single_hyp.trigram_prob], hyp, 
                [h.turk_translation.strip().lower().split() for h in hyps_for_one_sent])
            # features = [one_sentence_hyp.bigram_prob one_sentence_hyp.trigram_prob]

            # compute bleu score b
            bleu_score = mod_bleu.compute_bleu(hyp, ref[int(s)])
            # append (c, b) to nbests[i]
            candidate = hypothesis(s, single_hyp.turk_translation, bleu_score, features)
            # print "s is {} and bleu is {} and candidate is {}".format(s, bleu_score, candidate)
            # print "i = {} and hyp = {}".format(i, hyp)
            nbests[s].append(candidate)
        # print "Training went over {}".format(s)

    # print "nbests of length {}".format(len(nbests))
    j=0
    theta = [0 for i in xrange(len(nbests[0][0].features))]
    # print "theta {}".format(theta)

    for i in xrange(0, opts.epochs):
        seed(randint(0, 121))
        for nbest in nbests:
            j+=1
            # if len(nbest) == 1:
            # print "sampling with nbest {} for j {}".format(nbest, j/100)
            sample = get_sample(nbest, opts)
            # print "sample is {}".format(sample)
            sorted_sample = sorted(sample, key=lambda candidate: candidate[0].smoothed_bleu, reverse=True)
            top_sorted_sample = sorted_sample[0:opts.x_i]
            mistakes = 0
            for (s1, s2) in top_sorted_sample:
                # print "s1 features {} and \n s2 features {} and \n theta {}".format(s1.features, s2.features, theta)
                if vector_dot(theta, s1.features) <= vector_dot(theta, s2.features):
                    mistakes += 1
                    adj = [opts.eta * elem for elem in vector_diff(s1.features, s2.features)]
                    theta = [theta[i] + adj[i] for i in xrange(0, len(adj))]
    return theta

def rerank(all_hyps, theta):
    num_sents = len(all_hyps) / 4
    for s in xrange(0, num_sents):
        hyps_for_one_sent = all_hyps[s * 4:s * 4 + 4]
        (best_score, best) = (-1e300, '')
        for single_hyp in hyps_for_one_sent:
            hyp = single_hyp.turk_translation.strip().lower().split()

            features = create_features([single_hyp.bigram_prob, single_hyp.trigram_prob], hyp,
                [h.turk_translation.strip().lower().split() for h in hyps_for_one_sent])

            score = vector_dot(theta, features)

            if score > best_score:
                (best_score, best) = (score, single_hyp.turk_translation)
        try:
            sys.stdout.write("%s\n" % best)
        except (Exception):
            sys.exit(1)

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="../../data/training/turk_translations_w_logprob_eurparl_2.tsv", help="Turk translations")
optparser.add_option("-r", "--ref", dest="reference", default="../../data/training/LDCtranslations.tsv", help="LDC Translations")

optparser.add_option("-n", "--numofsentences", dest="n", default=80000, type="int", help="Number of sentences to run on")
optparser.add_option("-c", "--numoftrainingsentences", dest="t", default=40000, type="int", help="Number of sentences to run on")

optparser.add_option("-b", "--bigram", dest="bigram", default=-1.0, type="float", help="Language model weight for bigram")
optparser.add_option("-t", "--trigram", dest="trigram", default=-0.5, type="float", help="Translation model weight for trigram")

optparser.add_option("-a", "--alpha", dest="alpha", default=0.01, type="float", help="sampler acceptance cutoff")
optparser.add_option("-u", "--tau", dest="tau", default=5000, type="int", help="samples generated per input sentence")
optparser.add_option("-x", "--x_i", dest="x_i", default=100, type="int", help="training data generated from the samples tau")
optparser.add_option("-e", "--eta", dest="eta", default=0.5, type="float", help="perceptron learning rate")
optparser.add_option("-p", "--epochs", dest="epochs", default=5, type="int", help="number of epochs for perceptron training")

(opts, _) = optparser.parse_args()

hypothesis = namedtuple("hypothesis", "index, sentence, smoothed_bleu, features")

ref = parse.parse_references_from_file(opts.reference)[0:opts.n]

# all_hyps = [pair.split(' ||| ') for pair in open(opts.input)][0:opts.n]
all_hyps = parse.parse_translations_from_file(opts.input)[0:opts.n]

# print "all_hyps {} ref {}".format(all_hyps, ref)
weights = calculate_pro_weights(all_hyps[:1370], ref, opts)
print weights
rerank(all_hyps, weights)
# print langid.classify("This is a test")