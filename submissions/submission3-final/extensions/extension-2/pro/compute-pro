#!/usr/bin/env python
import optparse
from collections import namedtuple
import sys
import os
import math
import parse

import mod_bleu
from utility import *
from numpy import mean

#PRO Implementation

def calculate_pro_weights(all_hyps, ref, opts):
    num_sents = len(all_hyps) / 4
    nbests = [[] for _ in xrange(0, num_sents)]

    for s in xrange(0, num_sents):
        #collect all the n-best for i
        hyps_for_one_sent = all_hyps[s * 4:s * 4 + 4]

        nbests[s] = []
        for shyp in hyps_for_one_sent:

	    ## Feature groups 
            hyp = shyp.Translation.strip().lower().split()
	    features = [ shyp.Bigram_Lprob,shyp.Trigram_Lprob,shyp.Penalty_Short,shyp.Penalty_Long,shyp.Edit_Distance,shyp.Average_TER,shyp.Bigram_mismatch_percentage,shyp.Trigram_mismatch_percentage,shyp.Worker_Bigram_Lprob,shyp.Worker_Trigram_Lprob,shyp.Worker_Penalty_Short,shyp.Worker_Penalty_Long,shyp.Worker_Edit_Distance,shyp.Worker_Average_TER,shyp.Worker_Bigram_mismatch_percentage,shyp.Worker_Trigram_mismatch_percentage,shyp.IsEnglishNative,shyp.IsUrduNative,shyp.LocationIndia,shyp.LocationPakistan,shyp.YearSpeakingEnglish,shyp.YearSpeakingUrdu ]
	    #features = [ shyp.Bigram_Lprob,shyp.Trigram_Lprob,shyp.Penalty_Short,shyp.Penalty_Long,shyp.Edit_Distance,shyp.Average_TER,shyp.Bigram_mismatch_percentage,shyp.Trigram_mismatch_percentage ]
	    #features = [ shyp.Bigram_Lprob,shyp.Trigram_Lprob,shyp.Edit_Distance ]
	    #features = [ shyp.Worker_Bigram_Lprob,shyp.Worker_Trigram_Lprob,shyp.Worker_Penalty_Short,shyp.Worker_Penalty_Long,shyp.Worker_Edit_Distance,shyp.Worker_Average_TER,shyp.Worker_Bigram_mismatch_percentage,shyp.Worker_Trigram_mismatch_percentage,shyp.IsEnglishNative,shyp.IsUrduNative,shyp.LocationIndia,shyp.LocationPakistan,shyp.YearSpeakingEnglish,shyp.YearSpeakingUrdu ]
	    #features = [ shyp.Bigram_Lprob,shyp.Trigram_Lprob,shyp.Penalty_Short,shyp.Penalty_Long,shyp.Edit_Distance,shyp.Average_TER,shyp.Bigram_mismatch_percentage,shyp.Trigram_mismatch_percentage,shyp.Worker_Bigram_Lprob,shyp.Worker_Trigram_Lprob,shyp.Worker_Penalty_Short,shyp.Worker_Penalty_Long,shyp.Worker_Edit_Distance,shyp.Worker_Average_TER,shyp.Worker_Bigram_mismatch_percentage,shyp.Worker_Trigram_mismatch_percentage ]
	
	    # compute bleu score b
            bleu_score = mod_bleu.compute_bleu(hyp, ref[int(s)])
            candidate = hypothesis(s, shyp.Translation, bleu_score, features)
            nbests[s].append(candidate)

    j=0
    theta = [ randint(50,200) for i in xrange(len(nbests[0][0].features))]

    for i in xrange(0, opts.epochs):
        seed(randint(0, 121))
        for nbest in nbests:
            j+=1
            sample = get_sample(nbest, opts)
            sorted_sample = sorted(sample, key=lambda candidate: candidate[0].smoothed_bleu, reverse=True)
            top_sorted_sample = sorted_sample[0:opts.x_i]
            mistakes = 0
            for (s1, s2) in top_sorted_sample:
                if vector_dot(theta, s1.features) <= vector_dot(theta, s2.features):
                    mistakes += 1
		    adj = [opts.eta * elem for elem in vector_diff(s1.features, s2.features)]
                    theta = [theta[i] + adj[i] for i in xrange(0, len(adj))]
    return theta

def rerank(all_hyps, theta):
    num_sents = len(all_hyps) / 4
    for s in xrange(0, num_sents):
        hyps_for_one_sent = all_hyps[s * 4:s * 4 + 4]
        (best_score, best) = (-1e300, '')
        for shyp in hyps_for_one_sent:
            hyp = shyp.Translation.strip().lower().split()

	    ## Feature groups
	    features = [ shyp.Bigram_Lprob,shyp.Trigram_Lprob,shyp.Penalty_Short,shyp.Penalty_Long,shyp.Edit_Distance,shyp.Average_TER,shyp.Bigram_mismatch_percentage,shyp.Trigram_mismatch_percentage,shyp.Worker_Bigram_Lprob,shyp.Worker_Trigram_Lprob,shyp.Worker_Penalty_Short,shyp.Worker_Penalty_Long,shyp.Worker_Edit_Distance,shyp.Worker_Average_TER,shyp.Worker_Bigram_mismatch_percentage,shyp.Worker_Trigram_mismatch_percentage,shyp.IsEnglishNative,shyp.IsUrduNative,shyp.LocationIndia,shyp.LocationPakistan,shyp.YearSpeakingEnglish,shyp.YearSpeakingUrdu ]
	    #features = [ shyp.Bigram_Lprob,shyp.Trigram_Lprob,shyp.Edit_Distance ]
	    #features = [ shyp.Bigram_Lprob,shyp.Trigram_Lprob,shyp.Penalty_Short,shyp.Penalty_Long,shyp.Edit_Distance,shyp.Average_TER,shyp.Bigram_mismatch_percentage,shyp.Trigram_mismatch_percentage,shyp.Worker_Bigram_Lprob,shyp.Worker_Trigram_Lprob,shyp.Worker_Penalty_Short,shyp.Worker_Penalty_Long,shyp.Worker_Edit_Distance,shyp.Worker_Average_TER,shyp.Worker_Bigram_mismatch_percentage,shyp.Worker_Trigram_mismatch_percentage ] 
	    #features = [ shyp.Bigram_Lprob,shyp.Trigram_Lprob,shyp.Penalty_Short,shyp.Penalty_Long,shyp.Edit_Distance,shyp.Average_TER,shyp.Bigram_mismatch_percentage,shyp.Trigram_mismatch_percentage ]
	    #features = [ shyp.Worker_Bigram_Lprob,shyp.Worker_Trigram_Lprob,shyp.Worker_Penalty_Short,shyp.Worker_Penalty_Long,shyp.Worker_Edit_Distance,shyp.Worker_Average_TER,shyp.Worker_Bigram_mismatch_percentage,shyp.Worker_Trigram_mismatch_percentage,shyp.IsEnglishNative,shyp.IsUrduNative,shyp.LocationIndia,shyp.LocationPakistan,shyp.YearSpeakingEnglish,shyp.YearSpeakingUrdu ]
	    #features = [ shyp.Worker_Bigram_Lprob,shyp.Worker_Trigram_Lprob,shyp.Worker_Penalty_Short,shyp.Worker_Penalty_Long,shyp.Worker_Edit_Distance,shyp.Worker_Average_TER,shyp.Worker_Bigram_mismatch_percentage,shyp.Worker_Trigram_mismatch_percentage ]
	    #features = [ shyp.IsEnglishNative,shyp.IsUrduNative,shyp.LocationIndia,shyp.LocationPakistan,shyp.YearSpeakingEnglish,shyp.YearSpeakingUrdu ]

            score = vector_dot(theta, features)

            if score > best_score:
                (best_score, best) = (score, shyp.Translation)
        try:
            sys.stdout.write("%s\n" % best)
        except (Exception):
            sys.exit(1)

optparser = optparse.OptionParser()
optparser.add_option("-i", "--input", dest="input", default="data/turk_translations_features.tsv", help="Turk translations")
optparser.add_option("-r", "--ref", dest="reference", default="data/LDCtranslations.tsv", help="LDC Translations")

optparser.add_option("-n", "--numofsentences", dest="n", default=80000, type="int", help="Number of sentences to run on")
optparser.add_option("-c", "--numoftrainingsentences", dest="t", default=40000, type="int", help="Number of sentences to run on")

optparser.add_option("-b", "--bigram", dest="bigram", default=-1.0, type="float", help="Language model weight for bigram")
optparser.add_option("-t", "--trigram", dest="trigram", default=-0.5, type="float", help="Translation model weight for trigram")

optparser.add_option("-a", "--alpha", dest="alpha", default=0.01, type="float", help="sampler acceptance cutoff")
optparser.add_option("-u", "--tau", dest="tau", default=5000, type="int", help="samples generated per input sentence")
optparser.add_option("-x", "--x_i", dest="x_i", default=100, type="int", help="training data generated from the samples tau")
optparser.add_option("-e", "--eta", dest="eta", default=0.005, type="float", help="perceptron learning rate")
optparser.add_option("-p", "--epochs", dest="epochs", default=5, type="int", help="number of epochs for perceptron training")

(opts, _) = optparser.parse_args()

hypothesis = namedtuple("hypothesis", "index, sentence, smoothed_bleu, features")
ref = parse.parse_references_from_file(opts.reference)[0:opts.n]
all_hyps = parse.parse_translations_from_file_new(opts.input)[0:opts.n]
weights = calculate_pro_weights(all_hyps[:1370], ref, opts)
rerank(all_hyps, weights)
